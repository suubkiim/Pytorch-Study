{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "정보검색 과제 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suubkiim/Pytorch-practice/blob/master/%EC%A0%95%EB%B3%B4%EA%B2%80%EC%83%89_%EA%B3%BC%EC%A0%9C_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeFe5Mnu7UL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class TwoLayerNet(object):\n",
        "  \"\"\"\n",
        "  two-layer-perceptron.\n",
        "  Input dimension : N\n",
        "  Hidden layer dimension : H\n",
        "  Output dimension : C\n",
        "\n",
        "  Softmax loss function을 활용해 네트워크를 학습시킬 것입니다.\n",
        "  Hidden layer의 activation function으로는 ReLU를 사용합니다.\n",
        "\n",
        "  정리하자면, 네트워크는 다음과 같은 구조를 갖습니다.\n",
        "\n",
        "  input - linear layer - ReLU - linear layer - output\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
        "    \"\"\"\n",
        "    모델을 초기화하며 weight는 작은 랜덤값, bias는 0으로 초기화됩니다.\n",
        "    Weight와 bias는 self.params라는 dictionary에 저장됩니다.\n",
        "\n",
        "    W1: 첫 번째 layer의 weight; (D, H)\n",
        "    b1: 첫 번째 layer의 biase; (H,)\n",
        "    W2: 두 번째 layer의 weight; (H, C)\n",
        "    b2: 두 번째 layer의 biase; (C,)\n",
        "\n",
        "    Inputs:\n",
        "    - input_size: input data의 dimension.\n",
        "    - hidden_size: hidden layer의 neuron(node) 개수.\n",
        "    - output_size: output dimesion.\n",
        "    \"\"\"\n",
        "    self.params = {}\n",
        "    self.params['W1'] = std * torch.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = torch.zeros(hidden_size)\n",
        "    self.params['W2'] = std * torch.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = torch.zeros(output_size)\n",
        "\n",
        "  def loss(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Neural network의 loss와 gradient를 계산합니다.\n",
        "\n",
        "    Inputs:\n",
        "    - X: Input data. shape (N, D). 각각의 X[i]가 하나의 training sample이며 총 N개의 sample이 input으로 주어짐.\n",
        "    - y: Training label 벡터. y[i]는 X[i]에 대한 정수값의 label.\n",
        "      y가 주어질 경우 loss와 gradient를 반환하며 y가 주어지지 않으면 output을 반환\n",
        "\n",
        "    Returns:\n",
        "    y가 주어지지 않으면, shape (N, C)인 score matrix 반환\n",
        "    scores[i, c]는 input X[i]에 대한 class c의 score\n",
        "\n",
        "    y가 주어지면 (loss, grads) tuple 반환\n",
        "    loss: training batch에 대한 loss (scalar)\n",
        "    grads: {parameter 이름: gradient} 형태의 dictionary (self.params와 같은 키여야 함)\n",
        "    \"\"\"\n",
        "    # Dictionary에서 weight와 bias 불러오기\n",
        "    W1, b1 = self.params['W1'], self.params['b1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "    N, D = X.size()\n",
        "\n",
        "    # Forward path 계산\n",
        "    scores = None\n",
        "    #############################################################################\n",
        "    # TODO: Forward path를 수행하고, 'scores'에 결과값을 저장 (shape : (N, C))  #\n",
        "    #         input - linear layer - ReLU - linear layer - output               #\n",
        "    #############################################################################\n",
        "\n",
        "    def affine_relu_forward(x,w,b):\n",
        "      out_1= torch.add(torch.mm(x,w),b).clamp(0)\n",
        "      cache_1 = (x,w,b)\n",
        "      return out_1, cache_1\n",
        "\n",
        "    def affine_forward(x,w,b):\n",
        "      out_2 = torch.add(torch.mm(x,w),b)\n",
        "      cache_2 = (x,w,b)\n",
        "      return out_2, cache_2\n",
        "\n",
        "    out_1, cache_1 = affine_relu_forward(X, self.params['W1'], self.params['b1'])\n",
        "    out_2, cache_2 = affine_forward(out_1, self.params['W2'], self.params['b2'])\n",
        "    scores = out_2\n",
        "    \n",
        "    \n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "    \n",
        "    # 정답(target)이 주어지지 않은 경우 점수를 리턴하고 종료\n",
        "    if y is None:\n",
        "      return scores\n",
        "\n",
        "    # Loss 계산\n",
        "    loss = None\n",
        "    e = torch.exp(scores)\n",
        "    softmax = e / torch.sum(e, dim=1, keepdim=True)\n",
        "    #############################################################################\n",
        "    #       TODO: Output을 이용하여 loss값 계산하고, 'loss'에 저장(scalar)        #\n",
        "    #                loss function : negative log likelihood                    #\n",
        "    #              'softmax' 변수에 저장된 softmax값을 이용해서 계산              #\n",
        "    #         'y'는 정답 index를 가리키며 정답 확률에 -log 적용하여 평균           #\n",
        "    #############################################################################\n",
        "    N = X.shape[0]\n",
        "    loss = - torch.sum(torch.log(softmax[np.arange(N),y],dim=1,keepdim=True)) / N\n",
        "    dscores = softmax.copy()\n",
        "    dscores[np.arange(N),y] -=1\n",
        "    dscores /= N\n",
        "    \n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "\n",
        "    # Backward path(Gradient 계산) 구현\n",
        "    grads = {}\n",
        "    #############################################################################\n",
        "    # TODO: Weight와 bias에 대한 gradient를 계산하고 'grads' dictionary에 저장   #\n",
        "    #               dictionary의 key는 self.params와 동일하게 설정.             #\n",
        "    #          grads['W1']는 self.params['W1']과 같은 shape를 가져야 함.        #\n",
        "    #              softmax의 gradient부터 차근차근 구해나가도록 함.              #\n",
        "    #############################################################################\n",
        "    def affine_backward(gd, cache):\n",
        "      x, w, b = cache\n",
        "      dx, dw, db = None, None, None\n",
        "      \n",
        "      x = cache[0]\n",
        "      w = cache[1]\n",
        "      b = cache[2]\n",
        "      \n",
        "      dw = torch.mm(x.t(),gd)\n",
        "      dx = torch.mm(gd,w.t()).reshape(x.shape)\n",
        "      db = np.sum(gd, axis=0)\n",
        "      \n",
        "      return dx, dw, db\n",
        "    \n",
        "    def relu_backward(gd, cache) :\n",
        "      dx, x = None, cache\n",
        "      \n",
        "      x = cache\n",
        "      out = x.clamp(0) #ReLU  performed again\n",
        "      out[out>0] = 1\n",
        "      dx = out*gd\n",
        "      return dx\n",
        "    \n",
        "    def affine_relu_backward(gd, cache):\n",
        "      cache_0 = TwoLayer.netrelu_backward(gd,chace)\n",
        "      dx, dw, db = affine_backward(cache_0,cache)\n",
        "      return dx, dw, db\n",
        "    \n",
        "    dx_2, grads['W2'], grads['b2'] = affine_backward(dscores, cache_2)\n",
        "    dx_1, grads['W1'], grads['b1'] = affine_relu_backward(dx_2, cache_1) \n",
        "          \n",
        "    #############################################################################\n",
        "    #                              END OF YOUR CODE                             #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, grads\n",
        "\n",
        "  def train(self, X, y,\n",
        "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
        "            num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    \"\"\"\n",
        "    SGD를 이용한 neural network training\n",
        "\n",
        "    Inputs:\n",
        "    - X: shape (N, D)의 numpy array (training data)\n",
        "    - y: shape (N,)의 numpy array(training labels; y[i] = c\n",
        "                                  c는 X[i]의 label, 0 <= c < C)\n",
        "    - learning_rate: Scalar learning rate\n",
        "    - num_iters: Number of steps\n",
        "    - batch_size: Number of training examples in a mini-batch.\n",
        "    - verbose: true일 경우 progress 출력\n",
        "    \"\"\"\n",
        "    num_train = X.shape[0]\n",
        "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
        "\n",
        "    # SGD를 이용한 optimization\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    for it in range(num_iters):\n",
        "      loss, grads = self.loss(X, y=y)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      #########################################################################\n",
        "      # TODO: 'grads' dictionary에서 gradient를 불러와 SGD update 수행        #\n",
        "      #########################################################################\n",
        "      def SGD(w, dw, config=None):\n",
        "        if config is None: config = {}\n",
        "        config.setdefault('learning_rate', 1e-2)\n",
        "        \n",
        "        w -= config['learning_rate'] * dw\n",
        "        return w, config\n",
        "      \n",
        "      self.params['W1'],_ = SGD(self.params['W1'],grads['W1'],None)\n",
        "      self.params['W2'],_ = SGD(self.params['W2'],grads['W2'],None)\n",
        "      \n",
        "      #########################################################################\n",
        "      #                             END OF YOUR CODE                          #\n",
        "      #########################################################################\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "\n",
        "      if it % iterations_per_epoch == 0:\n",
        "        # Accuracy\n",
        "        train_acc = (self.predict(X) == y).float().mean()\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        learning_rate *= learning_rate_decay\n",
        "\n",
        "    return {\n",
        "      'loss_history': loss_history,\n",
        "      'train_acc_history': train_acc_history,\n",
        "      'val_acc_history': val_acc_history,\n",
        "    }\n",
        "\n",
        "  def predict(self, X):\n",
        "    return torch.argmax(self.loss(X),1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hV0gWfiQe3V",
        "colab_type": "code",
        "outputId": "a1b1286c-d729-4949-a52b-1636e4d12a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# A bit of setup\n",
        "\n",
        "import torch\n",
        "\n",
        "from NN_SB import TwoLayerNet\n",
        "\n",
        "# Create a small net and some toy data to check your implementations.\n",
        "# Note that we set the random seed for repeatable experiments.\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 10\n",
        "num_classes = 3\n",
        "num_inputs = 5\n",
        "\n",
        "def init_toy_model():\n",
        "    torch.manual_seed(0)\n",
        "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
        "\n",
        "def init_toy_data():\n",
        "    torch.manual_seed(1)\n",
        "    X = 10 * torch.randn(num_inputs, input_size)\n",
        "    y = torch.LongTensor([0, 1, 2, 2, 1])\n",
        "    return X, y\n",
        "\n",
        "net = init_toy_model()\n",
        "X, y = init_toy_data()\n",
        "\n",
        "scores = net.loss(X) \n",
        "print('Your scores:')\n",
        "print(scores)\n",
        "print()\n",
        "print('correct scores:')\n",
        "correct_scores = torch.Tensor(\n",
        "  [[ 0.24617445,  0.1261572,   1.1627575 ],\n",
        " [ 0.18364899, -0.0675799,  -0.21310908],\n",
        " [-0.2075074,  -0.12525336, -0.06508598],\n",
        " [ 0.08643292,  0.07172455,  0.2353122 ],\n",
        " [ 0.8219606,  -0.32560882, -0.77807254]]\n",
        ")\n",
        "print(correct_scores)\n",
        "print()\n",
        "\n",
        "print('Difference between your scores and correct scores:')\n",
        "print(torch.sum(torch.abs(scores - correct_scores)))\n",
        "\n",
        "loss, _ = net.loss(X, y)\n",
        "correct_loss = 1.2444149\n",
        "\n",
        "print('Difference between your loss and correct loss:')\n",
        "print(torch.sum(torch.abs(loss - correct_loss)))\n",
        "\n",
        "loss, grads = net.loss(X, y)\n",
        "\n",
        "results = net.train(X, y, 0.05)\n",
        "print(\"Train acc: %f -> %f\\nTrain loss: %f -> %f\" % (results['train_acc_history'][0], results['train_acc_history'][-1]\n",
        "                                                , results['loss_history'][0],results['loss_history'][-1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f2ca24b67ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_toy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your scores:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/NN_SB.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# 정답(target)이 주어지지 않은 경우 점수를 리턴하고 종료\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'affine_forward' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8LNJX6RCqIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "b750f863-ced8-47c0-8ca4-95aed4caa2e3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt0YmYC-L9QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}